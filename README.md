# ðŸ§  Reinforcement Learning Repository

Welcome to the **Reinforcement-Learning** repository by **Meri-07m**!  
This repository is a **comprehensive collection of reinforcement learning (RL) algorithms, environments, and experiments**, designed for learning, research, and exploration of key RL concepts. It includes classic RL benchmarks, function approximation techniques, exploration strategies, and visualization tools.

Whether you're new to RL or an advanced researcher, this repository provides **modular implementations, detailed examples, and visualizations** to study agent behaviors and algorithm performance.

---

## ðŸ“‚ Repository Structure

The repository contains the following submodules, each with its own focus:

### 1. `blackjack`
- Implements RL algorithms in the **Blackjack environment**.
- Includes policy evaluation, value iteration, and Monte Carlo methods.
- Serves as a classic example of decision-making under uncertainty.

### 2. `cliff-walking`
- Demonstrates RL in the **Cliff Walking environment**.
- Compares **Q-Learning** and **SARSA**.
- Visualizes optimal and suboptimal policies.

### 3. `coarse-coding`
- Explores **coarse coding** for function approximation in RL.
- Implements **tile coding** and **radial basis functions (RBFs)**.
- Supports Q-Learning and SARSA agents in continuous or large state spaces.

### 4. `gambler-problem`
- Models the **Gamblerâ€™s Problem** from dynamic programming.
- Demonstrates **policy evaluation and improvement** techniques.
- Provides insights into risk-sensitive decision-making.

### 5. `gridworld-dp`
- Implements **Dynamic Programming (DP)** in gridworlds.
- Features **value iteration** and **policy iteration**.
- Visualizes value functions and optimal policies.

### 6. `gridworld-mdp`
- Focuses on **Markov Decision Processes (MDPs)** in gridworlds.
- Includes stochastic transitions, rewards, and policy evaluation.
- Provides a theoretical foundation for RL algorithms.

### 7. `infinite-variance`
- Explores **RL scenarios with infinite variance rewards**.
- Investigates robust learning strategies and alternative approaches.

### 8. `mazes`
- Implements RL algorithms in **maze environments**.
- Supports Q-Learning, SARSA, and optionally DQN.
- Includes state visitation heatmaps, trajectory visualizations, and customizable mazes.

### 9. `random-walk-fa`
- Implements **Function Approximation (FA)** in random walk problems.
- Generalizes learning across large or continuous state spaces.

### 10. `random-walk-ntd`
- Studies **Random Walk problems using Natural Temporal Difference (NTD) learning**.
- Demonstrates convergence and trajectory visualization.

### 11. `random-walk`
- Classic **tabular random walk problem**.
- Helps study Monte Carlo and TD learning in small environments.

### 12. `ten-armed-testbed`
- Implements **Ten-Armed Bandit problems**.
- Evaluates exploration strategies such as epsilon-greedy and softmax.
- Supports stationary and non-stationary reward distributions.

### 13. `tic-tac-toe/src`
- Applies RL algorithms to the **Tic-Tac-Toe game**.
- Demonstrates Q-Learning and SARSA policy convergence.
- Ideal for studying RL in discrete action spaces.

### 14. `trajectory-sampling`
- Explores **trajectory sampling methods** for policy learning.
- Includes partial and full trajectory collection.
- Provides analysis of sampling strategies on learning efficiency.

### 15. `updates-comparison`
- Compares **different RL update rules**, including Q-Learning, SARSA, and Expected SARSA.
- Evaluates convergence speed, stability, and policy performance.

### 16. `windy-gridworld`
- Implements the **Windy Gridworld environment**.
- Introduces stochastic transitions to study exploration-exploitation trade-offs.
- Supports Q-Learning and SARSA agents with visualization.

---

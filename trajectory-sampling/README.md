# 🧭 Trajectory Sampling — Reinforcement Learning

This folder is part of the **[Reinforcement-Learning](https://github.com/Meri-07m/Reinforcement-Learning)** repository by **[Meri-07m](https://github.com/Meri-07m)**.  
It focuses on **trajectory sampling** — the process of collecting and analyzing agent–environment interactions for use in reinforcement learning algorithms.

---

## 🌍 Overview

In reinforcement learning (RL), a **trajectory** (or episode) is a sequence of state–action–reward transitions experienced by an agent as it interacts with an environment.

This module implements and compares different **trajectory sampling techniques**, allowing experiments in:

- 🌀 **On-policy vs Off-policy** sampling  
- 🎯 **Importance sampling** for re-weighting off-policy data  
- 📊 **Variance analysis** of sampled returns  
- 🔁 **Monte Carlo estimation** of returns  
- 🔍 **Trajectory visualization** (state visitation, reward flow, etc.)

Trajectory sampling is the foundation of methods like **Monte Carlo**, **Policy Gradient (REINFORCE)**, and **Actor–Critic** algorithms.

---


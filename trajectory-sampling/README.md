# ğŸ§­ Trajectory Sampling â€” Reinforcement Learning

This folder is part of the **[Reinforcement-Learning](https://github.com/Meri-07m/Reinforcement-Learning)** repository by **[Meri-07m](https://github.com/Meri-07m)**.  
It focuses on **trajectory sampling** â€” the process of collecting and analyzing agentâ€“environment interactions for use in reinforcement learning algorithms.

---

## ğŸŒ Overview

In reinforcement learning (RL), a **trajectory** (or episode) is a sequence of stateâ€“actionâ€“reward transitions experienced by an agent as it interacts with an environment.

This module implements and compares different **trajectory sampling techniques**, allowing experiments in:

- ğŸŒ€ **On-policy vs Off-policy** sampling  
- ğŸ¯ **Importance sampling** for re-weighting off-policy data  
- ğŸ“Š **Variance analysis** of sampled returns  
- ğŸ” **Monte Carlo estimation** of returns  
- ğŸ” **Trajectory visualization** (state visitation, reward flow, etc.)

Trajectory sampling is the foundation of methods like **Monte Carlo**, **Policy Gradient (REINFORCE)**, and **Actorâ€“Critic** algorithms.

---

